{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2414e468-5554-4052-894c-4458c02cfa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 2 of  mini project phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb0274-f87b-4003-801d-bc4142e2c219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\layer.py:391: UserWarning: `build()` was called on layer 'residual_unit', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\layer.py:391: UserWarning: `build()` was called on layer 'residual_unit_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\layer.py:391: UserWarning: `build()` was called on layer 'residual_unit_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Listening... (Press Ctrl+C to stop)\n",
      "Predicted Label: 10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_silence\n",
    "from scipy.signal import resample\n",
    "from io import BytesIO\n",
    "\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.signal import resample\n",
    "import csv\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "        # Define the custom ResidualUnit layer\n",
    "@register_keras_serializable()\n",
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.strides = strides\n",
    "        self.conv1 = layers.Conv2D(filters, kernel_size=3, strides=strides, padding=\"same\", use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.activation = layers.Activation(\"relu\")\n",
    "        self.conv2 = layers.Conv2D(filters, kernel_size=3, strides=1, padding=\"same\", use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        if strides > 1 or filters != kwargs.get('input_shape', [None, 374, 129, 1])[-1]:\n",
    "            self.skip_conv = layers.Conv2D(filters, kernel_size=1, strides=strides, padding=\"same\", use_bias=False)\n",
    "            self.skip_bn = layers.BatchNormalization()\n",
    "        else:\n",
    "            self.skip_conv = None\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "\n",
    "        if self.skip_conv is not None:\n",
    "            skip = self.skip_conv(inputs)\n",
    "            skip = self.skip_bn(skip, training=training)\n",
    "        else:\n",
    "            skip = inputs\n",
    "\n",
    "        return self.activation(x + skip)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"filters\": self.filters,\n",
    "            \"strides\": self.strides,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "model = tf.keras.models.load_model(\n",
    "    'farsi_numbers_detectionjupyter.keras',\n",
    "    custom_objects={'ResidualUnit': ResidualUnit}  # Include custom layers if used\n",
    ")\n",
    "commands = np.array(['8', '5', '4', '9', '1', '7', '6', '3', '2', '10', '0'])\n",
    "# Audio capture settings\n",
    "CHUNK = 1024  # Size of audio buffer\n",
    "FORMAT = pyaudio.paInt16  # Audio format\n",
    "CHANNELS = 1  # Mono audio\n",
    "RATE = 16000  # Sample rate (16 kHz)\n",
    "\n",
    "# Function to get MFCCs from audio\n",
    "def get_mfccs(audio, sample_rate):\n",
    "    frame_length = int(sample_rate / 40)  # 25 ms\n",
    "    frame_step = int(sample_rate / 100)  # 10 ms\n",
    "    fft_length = frame_length\n",
    "    num_feats = 40\n",
    "\n",
    "    stfts = tf.signal.stft(audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length)\n",
    "    spectrograms = tf.abs(stfts)\n",
    "\n",
    "    num_spectrogram_bins = stfts.shape[-1]\n",
    "    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 0, sample_rate / 2, num_feats\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "        num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz, upper_edge_hertz)\n",
    "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n",
    "\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)\n",
    "    mfccs = mfccs[..., tf.newaxis]  # Add a channel dimension\n",
    "    return mfccs\n",
    "\n",
    "# Process and predict a single audio chunk\n",
    "def process_and_predict_chunk(chunk, model):\n",
    "    # Export chunk to a temporary file\n",
    "    temp_chunk_path = 'temp_chunk.wav'\n",
    "    chunk.export(temp_chunk_path, format=\"wav\")\n",
    "\n",
    "    # Load and decode the chunk with TensorFlow\n",
    "    audio_binary = tf.io.read_file(temp_chunk_path)\n",
    "    audio, sample_rate = tf.audio.decode_wav(audio_binary)\n",
    "\n",
    "    # Handle multi-channel or single-channel audio\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = tf.reduce_mean(audio, axis=-1)  # Convert to mono if multi-channel\n",
    "    else:\n",
    "        audio = tf.squeeze(audio, axis=-1)  # Squeeze if single-channel\n",
    "\n",
    "    # Resample to 16 kHz if the sample rate is different\n",
    "    desired_sample_rate = 16000\n",
    "    if sample_rate.numpy() != desired_sample_rate:  # Convert tensor to numpy\n",
    "        num_samples = int(desired_sample_rate / sample_rate.numpy() * len(audio))\n",
    "        audio = resample(audio.numpy(), num_samples)\n",
    "        audio = tf.convert_to_tensor(audio, dtype=tf.float32)  # Convert back to tensor\n",
    "\n",
    "    # Get MFCCs\n",
    "    mfccs = get_mfccs(audio, desired_sample_rate)\n",
    "\n",
    "    # Ensure the MFCC shape matches the input shape expected by the model\n",
    "    input_shape = model.input_shape[1:]\n",
    "    mfccs = tf.image.resize(mfccs, [input_shape[0], input_shape[1]])  # Resize if necessary\n",
    "    mfccs = tf.expand_dims(mfccs, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Make a prediction\n",
    "    predictions = model.predict(mfccs)\n",
    "    predicted_label_index = np.argmax(predictions, axis=1)[0]\n",
    "    predicted_label = commands[predicted_label_index]\n",
    "\n",
    "    # Clean up the temporary file\n",
    "    os.remove(temp_chunk_path)\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# Real-time audio processing\n",
    "def realtime_prediction(model):\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Listening... (Press Ctrl+C to stop)\")\n",
    "\n",
    "    audio_buffer = AudioSegment.empty()\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Read data from the microphone\n",
    "            data = stream.read(CHUNK)\n",
    "            audio_segment = AudioSegment(data, sample_width=p.get_sample_size(FORMAT), frame_rate=RATE, channels=CHANNELS)\n",
    "            audio_buffer += audio_segment\n",
    "\n",
    "            # Detect silence and split audio\n",
    "            silence_ranges = detect_silence(audio_buffer, min_silence_len=500, silence_thresh=-50)\n",
    "\n",
    "            if silence_ranges:\n",
    "                # Process audio up to the first silence\n",
    "                start, end = silence_ranges[0]\n",
    "                chunk = audio_buffer[:end]\n",
    "                audio_buffer = audio_buffer[end:]  # Remove processed chunk from buffer\n",
    "\n",
    "                if start == 0:  # If silence is at the beginning\n",
    "                    predicted_label = '10'  # Unknown class\n",
    "                else:\n",
    "                    # Predict the label for the chunk\n",
    "                    predicted_label = process_and_predict_chunk(chunk, model)\n",
    "                \n",
    "                print(f\"Predicted Label: {predicted_label}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping...\")\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    realtime_prediction(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab17cb-5894-423e-92d2-86e1f5c504d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
