{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6af210ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d15ee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'build_model_mini.ipynb',\n",
       " 'mini_modelsaved',\n",
       " 'run.ipynb',\n",
       " 'test',\n",
       " 'Train']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "386d9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'Train/Train'\n",
    "\n",
    "data_dir = pathlib.Path(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6109e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Set the root directory containing the audio files\n",
    "root_dir = 'Train/Train'\n",
    "\n",
    "# Define the directory names\n",
    "directory_names = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "for dir_name in directory_names:\n",
    "    dir_path = os.path.join(root_dir, dir_name)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "# Move the audio files to their respective directories\n",
    "for filename in os.listdir(root_dir):\n",
    "    if filename.endswith('.wav'):\n",
    "        first_digit = int(filename[0])\n",
    "        src_path = os.path.join(root_dir, filename)\n",
    "        dst_path = os.path.join(root_dir, directory_names[first_digit], filename)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "        print(f\"Moved {filename} to directory {directory_names[first_digit]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcb9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import time\n",
    "\n",
    "def convert_to_mono(input_path, output_path):\n",
    "    y, sr = librosa.load(input_path, sr=None, mono=False)\n",
    "    if y.ndim > 1:\n",
    "        y = librosa.to_mono(y)\n",
    "    sf.write(output_path, y, sr)\n",
    "\n",
    "def preprocess_audio_files(root_directory):\n",
    "    start_time = time.time()\n",
    "    file_count = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                temp_path = os.path.join(root, 'temp_' + file)\n",
    "                \n",
    "                # Convert to mono if needed\n",
    "                convert_to_mono(file_path, temp_path)\n",
    "                \n",
    "                # Replace original file with the mono file\n",
    "                os.remove(file_path)\n",
    "                os.rename(temp_path, file_path)\n",
    "                \n",
    "                file_count += 1\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    print(f\"Processed {file_count} files in {total_time:.2f} seconds.\")\n",
    "    print(f\"Average time per file: {total_time / file_count:.2f} seconds.\")\n",
    "\n",
    "root_directory = 'Train/Train'\n",
    "preprocess_audio_files(root_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac5125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=data_dir,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    seed=0,\n",
    "    output_sequence_length=16000,\n",
    "    subset='both')\n",
    "\n",
    "label_names = np.array(train_ds.class_names)\n",
    "print()\n",
    "print(\"label names:\", label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze(audio, labels):\n",
    "  audio = tf.squeeze(audio, axis=-1)\n",
    "  return audio, labels\n",
    "\n",
    "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = val_ds.shard(num_shards=2, index=0)\n",
    "val_ds = val_ds.shard(num_shards=2, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85418eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_audio, example_labels in train_ds.take(1):\n",
    "    \n",
    "  print(example_audio.shape)\n",
    "  print(example_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d522ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "rows = 3\n",
    "cols = 3\n",
    "n = rows * cols\n",
    "for i in range(n):\n",
    "  plt.subplot(rows, cols, i+1)\n",
    "  audio_signal = example_audio[i]\n",
    "  plt.plot(audio_signal)\n",
    "  plt.title(label_names[example_labels[i]])\n",
    "  plt.yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "  plt.ylim([-1.1, 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "  # Convert the waveform to a spectrogram via a STFT.\n",
    "  spectrogram = tf.signal.stft(\n",
    "      waveform, frame_length=255, frame_step=128)\n",
    "  # Obtain the magnitude of the STFT.\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "  # Add a `channels` dimension, so that the spectrogram can be used\n",
    "  # as image-like input data with convolution layers (which expect\n",
    "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "  spectrogram = spectrogram[..., tf.newaxis]\n",
    "  return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "  label = label_names[example_labels[i]]\n",
    "  waveform = example_audio[i]\n",
    "  spectrogram = get_spectrogram(waveform)\n",
    "\n",
    "  print('Label:', label)\n",
    "  print('Waveform shape:', waveform.shape)\n",
    "  print('Spectrogram shape:', spectrogram.shape)\n",
    "  print('Audio playback')\n",
    "  display.display(display.Audio(waveform, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram, ax):\n",
    "  if len(spectrogram.shape) > 2:\n",
    "    assert len(spectrogram.shape) == 3\n",
    "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "  # Convert the frequencies to log scale and transpose, so that the time is\n",
    "  # represented on the x-axis (columns).\n",
    "  # Add an epsilon to avoid taking a log of zero.\n",
    "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "  height = log_spec.shape[0]\n",
    "  width = log_spec.shape[1]\n",
    "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "  Y = range(height)\n",
    "  ax.pcolormesh(X, Y, log_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "axes[0].plot(timescale, waveform.numpy())\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, 16000])\n",
    "\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "axes[1].set_title('Spectrogram')\n",
    "plt.suptitle(label.title())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spec_ds(ds):\n",
    "  return ds.map(\n",
    "      map_func=lambda audio,label: (get_spectrogram(audio), label),\n",
    "      num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spectrogram_ds = make_spec_ds(train_ds)\n",
    "val_spectrogram_ds = make_spec_ds(val_ds)\n",
    "test_spectrogram_ds = make_spec_ds(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e3f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba59af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
    "\n",
    "for i in range(n):\n",
    "    r = i // cols\n",
    "    c = i % cols\n",
    "    ax = axes[r][c]\n",
    "    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
    "    ax.set_title(label_names[example_spect_labels[i].numpy()])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
    "val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a95c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfdea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have used two max pool with pool size and strides 2.\n",
    "# Hence, downsampled feature maps are 16x smaller. The number of\n",
    "# filters in the last layer is 128. Reshape accordingly before\n",
    "# passing the output to the RNN part of the model\n",
    "\n",
    "input_shape = example_spectrograms.shape[1:]\n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(label_names)\n",
    "spect_width =124\n",
    "spect_height=129\n",
    "\n",
    "\n",
    "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
    "norm_layer = layers.Normalization()\n",
    "# Fit the state of the layer to the spectrograms\n",
    "# with `Normalization.adapt`.\n",
    "norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
    "new_shape = (spect_width // 16, (spect_height // 16) * 256)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "                              layers.Input(shape=input_shape),#256\n",
    "                              layers.Conv2D(32,(3, 3), activation=\"relu\", padding=\"same\"),\n",
    "                              layers.MaxPooling2D((2, 2)),#128\n",
    "                              layers.Conv2D(64,(3, 3), activation=\"relu\", padding=\"same\"),\n",
    "                              layers.MaxPooling2D((2, 2)),#64\n",
    "                              layers.Conv2D(128,(3, 3), activation=\"relu\", padding=\"same\"),\n",
    "                              layers.MaxPooling2D((2, 2)),#32\n",
    "                              layers.Conv2D(256,(3, 3), activation=\"relu\", padding=\"same\"),\n",
    "                              layers.MaxPooling2D((2, 2)),#16\n",
    "                              layers.Reshape(target_shape=new_shape),\n",
    "                              layers.Dense(128, activation=\"relu\"),\n",
    "                              layers.Dropout(0.2),\n",
    "                              layers.Conv1D(64, 3,activation=\"relu\", padding=\"same\"),\n",
    "                              layers.MaxPool1D(2),\n",
    "                              layers.Dropout(0.5),\n",
    "                              layers.Flatten(), # Add a Flatten layer to collapse the sequence dimension\n",
    "                              layers.Dense(num_labels, activation=\"softmax\")\n",
    "                              ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "# Assuming the same input_shape and num_labels as before\n",
    "input_shape = example_spectrograms.shape[1:]\n",
    "num_labels = len(label_names)\n",
    "\n",
    "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
    "norm_layer = layers.Normalization()\n",
    "# Fit the state of the layer to the spectrograms\n",
    "# with `Normalization.adapt`.\n",
    "norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
    "\n",
    "desired_sequence_length = 12544\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    # Downsample the input.\n",
    "    layers.Resizing(32, 32),\n",
    "    # Normalize.\n",
    "    norm_layer,\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Reshape((-1, desired_sequence_length)),  # Reshape to the desired sequence length\n",
    "    layers.TimeDistributed(layers.Dense(128, activation='relu')),\n",
    "    layers.LSTM(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_labels, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1019984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05afe842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d376e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9262ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "early_stopping_patience = 30\n",
    "reduce_lr_patience=10\n",
    "\n",
    "# Add early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\", patience=early_stopping_patience, restore_best_weights=True\n",
    ")\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=reduce_lr_patience)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_spectrogram_ds,\n",
    "    validation_data=val_spectrogram_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a21b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss [CrossEntropy]')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.ylim([0, 100])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy [%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead21ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_spectrogram_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_spectrogram_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82774812",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fadbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=label_names,\n",
    "            yticklabels=label_names,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b22499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportModel(tf.Module):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "    # Accept either a string-filename or a batch of waveforms.\n",
    "    # YOu could add additional signatures for a single wave, or a ragged-batch.\n",
    "    self.__call__.get_concrete_function(\n",
    "        x=tf.TensorSpec(shape=(), dtype=tf.string))\n",
    "    self.__call__.get_concrete_function(\n",
    "       x=tf.TensorSpec(shape=[None, 16000], dtype=tf.float32))\n",
    "\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, x):\n",
    "    # If they pass a string, load the file and decode it.\n",
    "    if x.dtype == tf.string:\n",
    "      x = tf.io.read_file(x)\n",
    "      x, _ = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
    "      x = tf.squeeze(x, axis=-1)\n",
    "      x = x[tf.newaxis, :]\n",
    "\n",
    "    x = get_spectrogram(x)\n",
    "    result = self.model(x, training=False)\n",
    "\n",
    "    class_ids = tf.argmax(result, axis=-1)\n",
    "    class_names = tf.gather(label_names, class_ids)\n",
    "    return {'predictions':result,\n",
    "            'class_ids': class_ids,\n",
    "            'class_names': class_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e46d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "export = ExportModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541e1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(export, \"mini_modelsaved\")\n",
    "imported = tf.saved_model.load(\"mini_modelsaved\")\n",
    "imported(waveform[tf.newaxis, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
